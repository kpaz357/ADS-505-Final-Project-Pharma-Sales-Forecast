{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245149ea-6ae5-4e53-a419-424c4681a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1dc8b6b-42cf-4552-aad0-5ca1b9d97859",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = Path(\"/Users/monicaekstein/Downloads/ADS505 Pharma Sales Data\")\n",
    "\n",
    "daily_raw = pd.read_csv(RAW_DIR / \"salesdaily.csv\")\n",
    "hourly_raw = pd.read_csv(RAW_DIR / \"saleshourly.csv\")\n",
    "weekly_raw = pd.read_csv(RAW_DIR / \"salesweekly.csv\")\n",
    "monthly_raw = pd.read_csv(RAW_DIR / \"salesmonthly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58efe7-811c-49ed-994a-15479a9cb216",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01375ee1-ddce-4c3e-a567-cf297156dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: 2014-03-30 → 2018-12-02  | Test range: 2018-12-09 → 2019-10-06\n",
      "Label rule: q95 (thr=589.84)\n",
      "Train: (245, 17), pos=13 (5.3%) | Test: (44, 17), pos=4 (9.1%)\n",
      "Built 4 time-aware CV fold(s).\n",
      "Best params: {'logit__C': 0.05, 'logit__penalty': 'l1'} | CV PR-AUC: 0.459\n",
      "Chosen decision threshold (CV median): τ=0.628\n",
      "[uncalibrated] ROC-AUC=0.888 | PR-AUC=0.392 | F1@τ=0.628 ⇒ 0.462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.971     0.850     0.907        40\n",
      "           1      0.333     0.750     0.462         4\n",
      "\n",
      "    accuracy                          0.841        44\n",
      "   macro avg      0.652     0.800     0.684        44\n",
      "weighted avg      0.913     0.841     0.866        44\n",
      "\n",
      "Confusion matrix:\n",
      " [[34  6]\n",
      " [ 1  3]]\n",
      "[calibrated] ROC-AUC=0.875 | PR-AUC=0.330 | F1@τ=0.628 ⇒ 0.286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.927     0.950     0.938        40\n",
      "           1      0.333     0.250     0.286         4\n",
      "\n",
      "    accuracy                          0.886        44\n",
      "   macro avg      0.630     0.600     0.612        44\n",
      "weighted avg      0.873     0.886     0.879        44\n",
      "\n",
      "Confusion matrix:\n",
      " [[38  2]\n",
      " [ 3  1]]\n",
      "[baseline] F1: 0.4444444444444444\n",
      "\n",
      "Top positive odds-ratio features:\n",
      " lag_1           2.141122\n",
      "lag_10          1.000000\n",
      "month           1.000000\n",
      "weekofyear      1.000000\n",
      "roll_mean_12    1.000000\n",
      "roll_mean_4     1.000000\n",
      "lag_12          1.000000\n",
      "lag_11          1.000000\n",
      "lag_9           1.000000\n",
      "lag_2           1.000000\n",
      "dtype: float64\n",
      "\n",
      "Top negative odds-ratio features:\n",
      " lag_11    1.0\n",
      "lag_9     1.0\n",
      "lag_2     1.0\n",
      "lag_8     1.0\n",
      "lag_7     1.0\n",
      "lag_6     1.0\n",
      "lag_5     1.0\n",
      "lag_4     1.0\n",
      "lag_3     1.0\n",
      "year      1.0\n",
      "dtype: float64\n",
      "\n",
      "Top FN (actual=1, predicted=0):\n",
      "        datum  y     p_cal  yhat\n",
      "2 2018-12-23  1  0.054735     0\n",
      "5 2019-01-13  1  0.106017     0\n",
      "6 2019-01-20  1  0.314815     0\n",
      "\n",
      "Top FP (actual=0, predicted=1):\n",
      "        datum  y  p_cal  yhat\n",
      "8 2019-02-03  0    1.0     1\n",
      "9 2019-02-10  0    1.0     1\n",
      "\n",
      "Estimated average uplift on surge weeks (train-only): 54.57%\n",
      "\n",
      "Saved artifacts → /Users/monicaekstein/Downloads/ADS505 Pharma Sales Data/artifacts\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "    f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "import json, os\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Setup\n",
    "try:\n",
    "    DATA_DIR = Path(RAW_DIR)\n",
    "except NameError:\n",
    "    DATA_DIR = Path(\".\")\n",
    "ARTIFACTS = DATA_DIR / \"artifacts\"\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load weekly and construct target series\n",
    "wk = pd.read_csv(DATA_DIR / \"salesweekly.csv\")\n",
    "wk.columns = [c.strip().lower().replace(\" \", \"_\") for c in wk.columns]\n",
    "wk[\"datum\"] = pd.to_datetime(wk[\"datum\"])\n",
    "value_cols = [c for c in wk.columns if c != \"datum\"] # all 8 ATC cols\n",
    "wk[\"units\"] = wk[value_cols].sum(axis=1)\n",
    "\n",
    "series = (wk.sort_values(\"datum\")\n",
    "            .set_index(\"datum\")[\"units\"]\n",
    "            .asfreq(\"W\", fill_value=0))  # regular weekly grid\n",
    "series.name = \"units\"\n",
    "\n",
    "# Feature engineering\n",
    "def make_features(s: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Plain lags/rolls + simple calendar\"\"\"\n",
    "    df = pd.DataFrame({\"units\": s})\n",
    "    for l in range(1, 13):\n",
    "        df[f\"lag_{l}\"] = s.shift(l)\n",
    "    df[\"roll_mean_4\"]  = s.rolling(4).mean()\n",
    "    df[\"roll_mean_12\"] = s.rolling(12).mean()\n",
    "    iso = df.index.isocalendar()\n",
    "    df[\"weekofyear\"] = iso.week.astype(int)\n",
    "    df[\"month\"] = df.index.month.astype(int)\n",
    "    df[\"year\"]  = df.index.year.astype(int)\n",
    "    df[\"units_next\"] = df[\"units\"].shift(-1)   # label base (no leakage)\n",
    "    return df.dropna()\n",
    "\n",
    "model_df = make_features(series).reset_index(names=\"datum\")\n",
    "model_df = model_df.sort_values(\"datum\").reset_index(drop=True)\n",
    "\n",
    "# 85/15 chronological split (TRAIN/TEST)\n",
    "cutoff_date = model_df[\"datum\"].quantile(0.85)  # ~early2014–mid2018 train, late2018–2019 test\n",
    "model_df[\"is_test\"] = model_df[\"datum\"] >= cutoff_date\n",
    "train_mask = ~model_df[\"is_test\"]\n",
    "\n",
    "print(f\"Train range: {model_df.loc[train_mask,'datum'].min().date()} → \"\n",
    "      f\"{model_df.loc[train_mask,'datum'].max().date()}  \"\n",
    "      f\"| Test range: {model_df.loc[~train_mask,'datum'].min().date()} → \"\n",
    "      f\"{model_df.loc[~train_mask,'datum'].max().date()}\")\n",
    "\n",
    "# Train-only labeling (leak-free)\n",
    "def label_surge_leak_free(model_df: pd.DataFrame, train_mask: pd.Series,\n",
    "                          min_pos_frac=0.02, min_pos_floor=5):\n",
    "    \"\"\"\n",
    "    Define 'surge_next' using TRAIN ONLY information.\n",
    "    \"\"\"\n",
    "    y_next_train = model_df.loc[train_mask, \"units_next\"]\n",
    "    min_pos = max(min_pos_floor, int(min_pos_frac * train_mask.sum()))\n",
    "    surge = pd.Series(0, index=model_df.index, dtype=int)\n",
    "    rule = None\n",
    "\n",
    "    # High-quantile on TRAIN\n",
    "    for q in [0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60]:\n",
    "        thr = float(np.quantile(y_next_train, q))\n",
    "        tmp = (model_df[\"units_next\"] >= thr).astype(int)\n",
    "        if tmp[train_mask].sum() >= min_pos:\n",
    "            surge = tmp; rule = f\"q{int(q*100)} (thr={thr:.2f})\"; break\n",
    "\n",
    "    # %-jump vs current week on TRAIN\n",
    "    if rule is None:\n",
    "        pct_jump = (model_df[\"units_next\"] - model_df[\"units\"]) / np.maximum(1.0, model_df[\"units\"])\n",
    "        for theta in [0.30, 0.25, 0.20, 0.15, 0.10]:\n",
    "            tmp = (pct_jump >= theta).astype(int)\n",
    "            if tmp[train_mask].sum() >= min_pos:\n",
    "                surge = tmp; rule = f\"pct_jump≥{int(theta*100)}%\"; break\n",
    "\n",
    "    # Top-k in TRAIN\n",
    "    if rule is None:\n",
    "        k = min_pos\n",
    "        idx_top = y_next_train.sort_values(ascending=False).index[:k]\n",
    "        surge.loc[idx_top] = 1\n",
    "        thr = float(y_next_train.loc[idx_top].min())\n",
    "        rule = f\"top-{k} by next-week units (min={thr:.2f})\"\n",
    "\n",
    "    return surge, rule\n",
    "\n",
    "surge, rule = label_surge_leak_free(model_df, train_mask)\n",
    "model_df[\"surge_next\"] = surge\n",
    "print(f\"Label rule: {rule}\")\n",
    "\n",
    "X_cols = [c for c in model_df.columns\n",
    "          if c not in {\"datum\", \"is_test\", \"units\", \"units_next\", \"surge_next\"}]\n",
    "\n",
    "# Final TRAIN/TEST frames\n",
    "train_df = model_df.loc[train_mask].reset_index(drop=True)\n",
    "test_df  = model_df.loc[~train_mask].reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train_df[X_cols], train_df[\"surge_next\"].astype(int)\n",
    "X_test,  y_test  = test_df[X_cols],  test_df[\"surge_next\"].astype(int)\n",
    "\n",
    "if y_train.nunique() < 2:\n",
    "    raise ValueError(\"Training set still 1-class; relax labeling or widen the train window.\")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, pos={y_train.sum()} ({y_train.mean():.1%}) | \"\n",
    "      f\"Test: {X_test.shape}, pos={y_test.sum()} ({y_test.mean():.1%})\")\n",
    "\n",
    "# Model + time-aware CV (on TRAIN only)\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logit\",  LogisticRegression(\n",
    "        solver=\"liblinear\", class_weight=\"balanced\",\n",
    "        max_iter=5000, random_state=SEED\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"logit__penalty\": [\"l2\", \"l1\"],\n",
    "    \"logit__C\": [0.05, 0.1, 0.25, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "# Expanding-window folds by week; skip folds with single class\n",
    "weeks = np.array(sorted(train_df[\"datum\"].unique()))\n",
    "W = len(weeks)\n",
    "n_splits = 4\n",
    "edges = np.linspace(0, W, num=n_splits + 2, dtype=int)\n",
    "\n",
    "splits = []\n",
    "for k in range(n_splits):\n",
    "    tr_weeks = weeks[:edges[k+1]]\n",
    "    te_weeks = weeks[edges[k+1]:edges[k+2]]\n",
    "    tr_idx = np.where(train_df[\"datum\"].isin(tr_weeks))[0]\n",
    "    te_idx = np.where(train_df[\"datum\"].isin(te_weeks))[0]\n",
    "    if tr_idx.size == 0 or te_idx.size == 0:\n",
    "        continue\n",
    "    if np.unique(y_train.iloc[tr_idx]).size < 2 or np.unique(y_train.iloc[te_idx]).size < 2:\n",
    "        continue\n",
    "    splits.append((tr_idx, te_idx))\n",
    "\n",
    "# Backup: internal 80/20 holdout inside TRAIN\n",
    "if not splits:\n",
    "    cut = int(0.8 * len(train_df))\n",
    "    tr_idx = np.arange(cut)\n",
    "    te_idx = np.arange(cut, len(train_df))\n",
    "    if np.unique(y_train.iloc[tr_idx]).size == 2 and np.unique(y_train.iloc[te_idx]).size == 2:\n",
    "        splits = [(tr_idx, te_idx)]\n",
    "    else:\n",
    "        raise ValueError(\"Could not form valid CV folds with both classes.\")\n",
    "\n",
    "print(f\"Built {len(splits)} time-aware CV fold(s).\")\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",   # PR-AUC is the right thing under imbalance\n",
    "    cv=splits,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=0,\n",
    "    error_score=0.0\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "best = gs.best_estimator_\n",
    "print(\"Best params:\", gs.best_params_, \"| CV PR-AUC:\", f\"{gs.best_score_:.3f}\")\n",
    "\n",
    "# Pick threshold on CV (TRAIN only)\n",
    "def best_tau_for(y_true, p):\n",
    "    pr, rc, thr = precision_recall_curve(y_true, p)\n",
    "    if len(thr) == 0:  # degenerate\n",
    "        return 0.5\n",
    "    f1 = 2*pr*rc/(pr+rc+1e-12)\n",
    "    return float(thr[np.argmax(f1[:-1])])\n",
    "\n",
    "taus = []\n",
    "for tr_idx, te_idx in splits:\n",
    "    # re-fit best params on each fold's train\n",
    "    fold_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logit\",  LogisticRegression(\n",
    "            solver=\"liblinear\",\n",
    "            penalty=best.named_steps[\"logit\"].penalty,\n",
    "            C=best.named_steps[\"logit\"].C,\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=5000,\n",
    "            random_state=SEED\n",
    "        )),\n",
    "    ])\n",
    "    fold_pipe.fit(X_train.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "    p_te = fold_pipe.predict_proba(X_train.iloc[te_idx])[:,1]\n",
    "    if len(np.unique(y_train.iloc[te_idx])) == 2:\n",
    "        taus.append(best_tau_for(y_train.iloc[te_idx], p_te))\n",
    "\n",
    "tau_cv = float(np.median(taus)) if taus else 0.5\n",
    "print(f\"Chosen decision threshold (CV median): τ={tau_cv:.3f}\")\n",
    "\n",
    "# Probability calibration (fit on TRAIN)\n",
    "try:\n",
    "    calibrated = CalibratedClassifierCV(estimator=best, method=\"isotonic\", cv=3)\n",
    "except TypeError:\n",
    "    calibrated = CalibratedClassifierCV(base_estimator=best, method=\"isotonic\", cv=3)\n",
    "\n",
    "try:\n",
    "    calibrated.fit(X_train, y_train)\n",
    "    p_test_cal = calibrated.predict_proba(X_test)[:,1]\n",
    "except Exception:\n",
    "    try:\n",
    "        calibrated = CalibratedClassifierCV(estimator=best, method=\"sigmoid\", cv=3)\n",
    "    except TypeError:\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=best, method=\"sigmoid\", cv=3)\n",
    "    calibrated.fit(X_train, y_train)\n",
    "    p_test_cal = calibrated.predict_proba(X_test)[:,1]\n",
    "\n",
    "p_test_raw = best.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Metrics on TEST (robust to single-class)\n",
    "def summarize(y, p, tau, tag):\n",
    "    metrics = {\"tag\": tag}\n",
    "    has_both = (np.unique(y).size == 2)\n",
    "    if has_both:\n",
    "        roc = roc_auc_score(y, p)\n",
    "        prauc = average_precision_score(y, p)\n",
    "        yhat = (p >= tau).astype(int)\n",
    "        f1 = f1_score(y, yhat)\n",
    "        print(f\"[{tag}] ROC-AUC={roc:.3f} | PR-AUC={prauc:.3f} | F1@τ={tau:.3f} ⇒ {f1:.3f}\")\n",
    "        print(classification_report(y, yhat, digits=3))\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y, yhat))\n",
    "        metrics.update({\"roc_auc\": float(roc), \"pr_auc\": float(prauc), \"f1\": float(f1)})\n",
    "    else:\n",
    "        pos = int(y.sum())\n",
    "        print(f\"[{tag}] Test has a single class (positives={pos}). \"\n",
    "              \"ROC/PR/F1 undefined; skipping threshold metrics.\")\n",
    "        metrics.update({\"roc_auc\": None, \"pr_auc\": None, \"f1\": None})\n",
    "    return metrics\n",
    "\n",
    "m_raw = summarize(y_test, p_test_raw, tau_cv, \"uncalibrated\")\n",
    "m_cal = summarize(y_test, p_test_cal, tau_cv, \"calibrated\")\n",
    "\n",
    "# Sanity baseline (on TEST)\n",
    "pct90 = np.nanpercentile(train_df[\"lag_1\"], 90)\n",
    "yhat_base = (X_test[\"lag_1\"] >= pct90).astype(int)\n",
    "if np.unique(y_test).size == 2:\n",
    "    print(\"[baseline] F1:\", f1_score(y_test, yhat_base))\n",
    "else:\n",
    "    print(\"[baseline] skipped (test single-class)\")\n",
    "\n",
    "# Coefficients (odds ratios)\n",
    "coef = best.named_steps[\"logit\"].coef_.ravel()\n",
    "odds = pd.Series(np.exp(coef), index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop positive odds-ratio features:\\n\", odds.head(10))\n",
    "print(\"\\nTop negative odds-ratio features:\\n\", odds.tail(10))\n",
    "\n",
    "# Error analysis + surge watchlist (on TEST)\n",
    "err = test_df[[\"datum\"]].copy()\n",
    "err[\"y\"] = y_test.to_numpy()\n",
    "err[\"p_cal\"] = p_test_cal\n",
    "err[\"yhat\"] = (err[\"p_cal\"] >= tau_cv).astype(int)\n",
    "fn = err[(err[\"y\"] == 1) & (err[\"yhat\"] == 0)].sort_values(\"p_cal\").head(5)\n",
    "fp = err[(err[\"y\"] == 0) & (err[\"yhat\"] == 1)].sort_values(\"p_cal\", ascending=False).head(5)\n",
    "print(\"\\nTop FN (actual=1, predicted=0):\\n\", fn)\n",
    "print(\"\\nTop FP (actual=0, predicted=1):\\n\", fp)\n",
    "\n",
    "watchlist = err.sort_values(\"p_cal\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Train-only surge uplift\n",
    "mu_pos = train_df.loc[train_df[\"surge_next\"] == 1, \"units_next\"].mean()\n",
    "mu_neg = train_df.loc[train_df[\"surge_next\"] == 0, \"units_next\"].mean()\n",
    "uplift = float(max(0.0, (mu_pos - mu_neg) / max(mu_neg, 1e-6)))\n",
    "print(f\"\\nEstimated average uplift on surge weeks (train-only): {uplift:.2%}\")\n",
    "\n",
    "# Save artifacts\n",
    "meta = {\n",
    "    \"best_params\": gs.best_params_,\n",
    "    \"cv_pr_auc\": float(gs.best_score_),\n",
    "    \"tau_cv\": float(tau_cv),›\n",
    "    \"cutoff_date\": str(cutoff_date.date()),\n",
    "    \"test_rows\": int(len(test_df)),\n",
    "    \"test_pos\": int(y_test.sum()),\n",
    "    \"label_rule\": str(rule),\n",
    "    \"uplift_pct\": uplift,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "(pd.Series(meta, dtype=\"object\")\n",
    "   .to_json(ARTIFACTS / \"logit_meta.json\", indent=2))\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"date\": test_df[\"datum\"],\n",
    "    \"y\": y_test,\n",
    "    \"p_raw\": p_test_raw,\n",
    "    \"p_cal\": p_test_cal,\n",
    "    \"yhat\": (p_test_cal >= tau_cv).astype(int)\n",
    "}).to_csv(ARTIFACTS / \"logit_test_preds.csv\", index=False)\n",
    "\n",
    "odds.to_csv(ARTIFACTS / \"logit_odds_ratio.csv\")\n",
    "watchlist[[\"datum\",\"y\",\"p_cal\",\"yhat\"]].to_csv(ARTIFACTS / \"logit_watchlist.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved artifacts → {ARTIFACTS.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab4ee0-6100-4b69-9422-ad8726f880be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
